{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a continuation of the **Monet-cyclegan-1**. I will continue training the model with a decayed learning rate and early stopping setup, then submit the newly generated images to Kaggle to see if the results improve.\n",
    "\n",
    "Since this is a separate notebook, I will load the data and build the model as I did in **Monet-cyclegan-1**., and then load the weights from the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:57:26.357457Z",
     "iopub.status.busy": "2025-04-04T03:57:26.357124Z",
     "iopub.status.idle": "2025-04-04T03:57:37.882789Z",
     "shell.execute_reply": "2025-04-04T03:57:37.881882Z",
     "shell.execute_reply.started": "2025-04-04T03:57:26.357426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages \n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Input, Conv2D, Conv2DTranspose, GroupNormalization \n",
    "from keras.layers import Activation, ReLU, LeakyReLU, Add\n",
    "from keras.optimizers import Adam\n",
    "from keras.ops import pad, ones_like, zeros_like\n",
    "from keras.utils import register_keras_serializable\n",
    "from keras.losses import MeanSquaredError, MeanAbsoluteError\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers.schedules import PolynomialDecay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:58:41.958281Z",
     "iopub.status.busy": "2025-04-04T03:58:41.957678Z",
     "iopub.status.idle": "2025-04-04T03:58:41.964103Z",
     "shell.execute_reply": "2025-04-04T03:58:41.963228Z",
     "shell.execute_reply.started": "2025-04-04T03:58:41.958248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to decode images from TFRecord files\n",
    "def decode_img(img):\n",
    "    image = tf.io.decode_jpeg(img, channels = 3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [256, 256, 3])\n",
    "    return image\n",
    "\n",
    "# Define a function to read images from TFRecord files\n",
    "def read_tfrecord(tfdata):\n",
    "    tfrecord_format = {\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    tfdata = tf.io.parse_single_example(tfdata, tfrecord_format)\n",
    "    image = decode_img(tfdata['image'])\n",
    "    return image\n",
    "\n",
    "# Define a function to load images from the TFRecord files\n",
    "def load_dataset(filenames):\n",
    "    # disable order, increase speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False   \n",
    "    \n",
    "    # read images from multiple files if available\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(read_tfrecord)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:58:54.351636Z",
     "iopub.status.busy": "2025-04-04T03:58:54.351321Z",
     "iopub.status.idle": "2025-04-04T03:58:55.408363Z",
     "shell.execute_reply": "2025-04-04T03:58:55.407521Z",
     "shell.execute_reply.started": "2025-04-04T03:58:54.351615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the file names from monet_tfrec and photo_tfrec\n",
    "monet_file = tf.io.gfile.glob('/kaggle/input/gan-getting-started/monet_tfrec/*.tfrec')\n",
    "photo_file = tf.io.gfile.glob('/kaggle/input/gan-getting-started/photo_tfrec/*.tfrec')\n",
    "\n",
    "# Load the datasets\n",
    "monet_ds = load_dataset(monet_file).batch(1)\n",
    "photo_ds = load_dataset(photo_file).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:58:55.409936Z",
     "iopub.status.busy": "2025-04-04T03:58:55.409646Z",
     "iopub.status.idle": "2025-04-04T03:58:55.423073Z",
     "shell.execute_reply": "2025-04-04T03:58:55.422203Z",
     "shell.execute_reply.started": "2025-04-04T03:58:55.409908Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to prepare the data \n",
    "def CycleGan_dataset(monet_dataset, photo_dataset):\n",
    "    monet_ds = monet_dataset.repeat()\n",
    "    photo_ds = photo_dataset.repeat()\n",
    "   \n",
    "    monet_ds = monet_ds.shuffle(2048)\n",
    "    photo_ds = photo_ds.shuffle(2048)\n",
    "    \n",
    "    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))\n",
    "    \n",
    "    return gan_ds\n",
    "\n",
    "# Get the training set \n",
    "CycleGan_ds = CycleGan_dataset(monet_ds, photo_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CycleGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:58:59.456178Z",
     "iopub.status.busy": "2025-04-04T03:58:59.455889Z",
     "iopub.status.idle": "2025-04-04T03:58:59.519482Z",
     "shell.execute_reply": "2025-04-04T03:58:59.518846Z",
     "shell.execute_reply.started": "2025-04-04T03:58:59.456157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Weights initializer for the layers\n",
    "kernel_init = keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n",
    "\n",
    "# Weights initializer for the instance normalization\n",
    "gamma_init = keras.initializers.RandomNormal(mean = 0.0, stddev = 0.02)\n",
    "\n",
    "# Define a function for downsampling block\n",
    "def downsample(x, filters, kernel_size, strides, activation = 'relu'):\n",
    "    x = Conv2D(filters, kernel_size, strides = strides, padding = 'same',\n",
    "                     kernel_initializer = kernel_init, use_bias = False)(x)\n",
    "    x = GroupNormalization(groups = filters, gamma_initializer = gamma_init)(x)    # Instance Normalization\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define a function for upsampling block\n",
    "def upsample(x, filters, kernel_size, strides, activation = 'relu'):\n",
    "    x = Conv2DTranspose(filters, kernel_size, strides = strides, padding = 'same',\n",
    "                     kernel_initializer = kernel_init, use_bias = False)(x)\n",
    "    x = GroupNormalization(groups = filters, gamma_initializer = gamma_init)(x)  \n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "# Define a layer of reflection padding via subclassing\n",
    "@register_keras_serializable()\n",
    "class reflection_padding(Layer):\n",
    "    def __init__(self, padding = (1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, input_tensor):\n",
    "        pad_x, pad_y = self.padding\n",
    "        pad_width = [\n",
    "            [0, 0],              # no padding on batch axis\n",
    "            [pad_y, pad_y],      # padding on image height axis\n",
    "            [pad_x, pad_x],      # padding on image width axis\n",
    "            [0, 0]               # no padding on channel axis\n",
    "        ]\n",
    "        return pad(input_tensor, pad_width, mode = \"reflect\")\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super.get_config()\n",
    "        config.update({\"padding\": self.padding})\n",
    "        return config\n",
    "\n",
    "# Define a layer of residule block via subclassing\n",
    "@register_keras_serializable()\n",
    "class residual_block(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.padding = reflection_padding()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        filters = input_shape[-1]\n",
    "        self.conv1 = Conv2D(filters, kernel_size = (3, 3), \n",
    "                            kernel_initializer = kernel_init, use_bias = False)\n",
    "        self.conv2 = Conv2D(filters, kernel_size = (3, 3), \n",
    "                            kernel_initializer = kernel_init, use_bias = False)\n",
    "        self.instance_norm1 = GroupNormalization(groups = filters, gamma_initializer = gamma_init)\n",
    "        self.instance_norm2 = GroupNormalization(groups = filters, gamma_initializer = gamma_init)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_tensor = inputs\n",
    "        # first conv block\n",
    "        x = self.padding(inputs)\n",
    "        x = self.conv1(x)\n",
    "        x = self.instance_norm1(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # second conv block\n",
    "        x = self.padding(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.instance_norm2(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "        # output\n",
    "        x = Add()([input_tensor, x])\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        return super.get_config()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:01.553786Z",
     "iopub.status.busy": "2025-04-04T03:59:01.553495Z",
     "iopub.status.idle": "2025-04-04T03:59:01.560153Z",
     "shell.execute_reply": "2025-04-04T03:59:01.559230Z",
     "shell.execute_reply.started": "2025-04-04T03:59:01.553765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the generator\n",
    "def generator(filters = 64, num_downsample = 2, num_residual = 9, num_upsample = 2, name = None):\n",
    "    inputs = Input(shape = [256, 256, 3])\n",
    "    # First convolutional block \n",
    "    x = reflection_padding(padding = (3, 3))(inputs)\n",
    "    x = Conv2D(filters, kernel_size = (7, 7), kernel_initializer = kernel_init, use_bias = False)(x)\n",
    "    x = GroupNormalization(groups = filters, gamma_initializer = gamma_init)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Downsampling block\n",
    "    for _ in range(num_downsample):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters, (3, 3), (2, 2), activation = 'relu')\n",
    "\n",
    "    # Residual block\n",
    "    for _ in range(num_residual):\n",
    "        x = residual_block()(x)\n",
    "\n",
    "    # Upsampling block\n",
    "    for _ in range(num_upsample):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, (3, 3), (2, 2), activation = 'relu')\n",
    "\n",
    "    # Final block\n",
    "    x = reflection_padding(padding = (3, 3))(x)\n",
    "    x = Conv2D(3, kernel_size = (7, 7), kernel_initializer = kernel_init, use_bias = False)(x)\n",
    "    x = GroupNormalization(groups = 3, gamma_initializer = gamma_init)(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    return Model(inputs = inputs, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:02.139695Z",
     "iopub.status.busy": "2025-04-04T03:59:02.139350Z",
     "iopub.status.idle": "2025-04-04T03:59:02.144995Z",
     "shell.execute_reply": "2025-04-04T03:59:02.143959Z",
     "shell.execute_reply.started": "2025-04-04T03:59:02.139671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the discriminator\n",
    "def discriminator(filters = 64, num_downsample = 3, name = None):\n",
    "    inputs = Input(shape = [256, 256, 3])\n",
    "    # First convolutional block\n",
    "    x = Conv2D(filters, kernel_size = (4, 4), strides = (2, 2), padding = 'same',\n",
    "               kernel_initializer = kernel_init, use_bias = False)(inputs)\n",
    "    x = Activation('leaky_relu')(x)\n",
    "\n",
    "    # Downsampling block\n",
    "    for _ in range(num_downsample):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters, (4, 4), (2, 2), activation = 'leaky_relu')   \n",
    "\n",
    "    # Final block\n",
    "    x = Conv2D(1, kernel_size = (4, 4), padding = 'same',\n",
    "               kernel_initializer = kernel_init, use_bias = False)(x)\n",
    "\n",
    "    return Model(inputs = inputs, outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:02.638743Z",
     "iopub.status.busy": "2025-04-04T03:59:02.638413Z",
     "iopub.status.idle": "2025-04-04T03:59:02.643616Z",
     "shell.execute_reply": "2025-04-04T03:59:02.642741Z",
     "shell.execute_reply.started": "2025-04-04T03:59:02.638717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the functions of adversarial loss\n",
    "def generator_loss(dis_fake):\n",
    "    loss_fn = MeanSquaredError()\n",
    "    return loss_fn(ones_like(dis_fake), dis_fake)\n",
    "\n",
    "def discriminator_loss(dis_real, dis_fake):\n",
    "    loss_fn = MeanSquaredError()\n",
    "    real_loss = loss_fn(ones_like(dis_real), dis_real)\n",
    "    fake_loss = loss_fn(zeros_like(dis_fake), dis_fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "# Define the function of cycle consistency loss\n",
    "def cycle_loss(img, cycled_img, Lambda):\n",
    "    loss_fn = MeanAbsoluteError()\n",
    "    return Lambda * loss_fn(img, cycled_img)\n",
    "\n",
    "# Define the function of identity loss\n",
    "def identity_loss(img, same_img, Lambda):\n",
    "    loss_fn = MeanAbsoluteError()\n",
    "    return 0.5 * Lambda *  loss_fn(img, same_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:03.323530Z",
     "iopub.status.busy": "2025-04-04T03:59:03.323173Z",
     "iopub.status.idle": "2025-04-04T03:59:03.333633Z",
     "shell.execute_reply": "2025-04-04T03:59:03.332642Z",
     "shell.execute_reply.started": "2025-04-04T03:59:03.323493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the CycleGAN model via subclassing\n",
    "class CycleGAN(Model):\n",
    "    def __init__(self, generator_monet, generator_photo, discriminator_monet, \n",
    "                 discriminator_photo, lambda_cycle = 10):\n",
    "        super().__init__()\n",
    "        self.GenM = generator_monet\n",
    "        self.GenP = generator_photo\n",
    "        self.DisM = discriminator_monet\n",
    "        self.DisP = discriminator_photo\n",
    "        self.Lamd = lambda_cycle\n",
    "        \n",
    "    def compile(self, \n",
    "                genM_optimizer, genP_optimizer, disM_optimizer, disP_optimizer,\n",
    "                gen_loss, dis_loss, cycle_loss, identity_loss):\n",
    "        super().compile()\n",
    "        self.GenM_Opt = genM_optimizer\n",
    "        self.GenP_Opt = genP_optimizer\n",
    "        self.DisM_Opt = disM_optimizer\n",
    "        self.DisP_Opt = disP_optimizer\n",
    "        self.gen_loss = gen_loss\n",
    "        self.dis_loss = dis_loss\n",
    "        self.cycle_loss = cycle_loss\n",
    "        self.identity_loss = identity_loss\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        real_monet, real_photo = batch_data\n",
    "\n",
    "        with tf.GradientTape(persistent = True) as tape:\n",
    "            # photo → monet → photo\n",
    "            fake_monet = self.GenM(real_photo, training = True)\n",
    "            cycled_photo = self.GenP(fake_monet, training = True)\n",
    "\n",
    "            # monet → photo → monet\n",
    "            fake_photo = self.GenP(real_monet, training = True)\n",
    "            cycled_monet = self.GenM(fake_photo, training = True)\n",
    "\n",
    "            # identity mapping\n",
    "            same_monet = self.GenM(real_monet, training = True)\n",
    "            same_photo = self.GenP(real_photo, training = True)\n",
    "            \n",
    "            # discriminator output\n",
    "            dis_real_monet = self.DisM(real_monet, training = True)\n",
    "            dis_fake_monet = self.DisM(fake_monet, training = True)\n",
    "            dis_real_photo = self.DisP(real_photo, training = True)\n",
    "            dis_fake_photo = self.DisP(fake_photo, training = True)\n",
    "\n",
    "            # generator adversarial loss\n",
    "            genM_adver = self.gen_loss(dis_fake_monet)\n",
    "            genP_adver = self.gen_loss(dis_fake_photo)\n",
    "\n",
    "            # cycle loss\n",
    "            cycle_photo = self.cycle_loss(real_photo, cycled_photo, self.Lamd)\n",
    "            cycle_monet = self.cycle_loss(real_monet, cycled_monet, self.Lamd)\n",
    "            total_cycle = cycle_photo + cycle_monet\n",
    "\n",
    "            # identity loss\n",
    "            genM_identity = self.identity_loss(real_monet, same_monet, self.Lamd)\n",
    "            genP_identity = self.identity_loss(real_photo, same_photo, self.Lamd)\n",
    "\n",
    "            # total generator loss\n",
    "            genM_loss = genM_adver + total_cycle + genM_identity\n",
    "            genP_loss = genP_adver + total_cycle + genP_identity\n",
    "\n",
    "            # discriminator loss\n",
    "            disM_loss = self.dis_loss(dis_real_monet, dis_fake_monet)\n",
    "            disP_loss = self.dis_loss(dis_real_photo, dis_fake_photo)\n",
    "\n",
    "        # calculate gradients for generators\n",
    "        grads_GenM = tape.gradient(genM_loss, self.GenM.trainable_variables)\n",
    "        grads_GenP = tape.gradient(genP_loss, self.GenP.trainable_variables)\n",
    "            \n",
    "        # calculate gradients for discriminators\n",
    "        grads_DisM = tape.gradient(disM_loss, self.DisM.trainable_variables)\n",
    "        grads_DisP = tape.gradient(disP_loss, self.DisP.trainable_variables)\n",
    "\n",
    "        # update weights of generators and discriminators\n",
    "        self.GenM_Opt.apply_gradients(zip(grads_GenM, self.GenM.trainable_variables))\n",
    "        self.GenP_Opt.apply_gradients(zip(grads_GenP, self.GenP.trainable_variables))\n",
    "        self.DisM_Opt.apply_gradients(zip(grads_DisM, self.DisM.trainable_variables))\n",
    "        self.DisP_Opt.apply_gradients(zip(grads_DisP, self.DisP.trainable_variables))\n",
    "\n",
    "        return {\n",
    "            \"genM_loss\": genM_loss,\n",
    "            \"genP_loss\": genP_loss,\n",
    "            \"disM_loss\": disM_loss,\n",
    "            \"disP_loss\": disP_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue training\n",
    "\n",
    "In the original paper, the model was trained for 200 epochs, but I only trained my first model for 50 epochs. In this session, I will load the weights from the first model, and continue training for another 150 epochs with the following setup: \n",
    "\n",
    "* Use a learning rate schedule: keep the learning rate at 0.0002 for the first 100 epochs, then linearly decay the rate to zero over the next 100 epochs.\n",
    "* Add an early stopping: stop training if the loss of `generator_monet` doesn't decrease for 5 consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:03.806583Z",
     "iopub.status.busy": "2025-04-04T03:59:03.806246Z",
     "iopub.status.idle": "2025-04-04T03:59:03.810552Z",
     "shell.execute_reply": "2025-04-04T03:59:03.809744Z",
     "shell.execute_reply.started": "2025-04-04T03:59:03.806554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define model check_point\n",
    "check_point = ModelCheckpoint(\n",
    "    filepath = '/kaggle/working/cyclegan2.weights.h5',\n",
    "    save_weights_only = True\n",
    ")\n",
    "\n",
    "# Define Early Stopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = 'genM_loss',\n",
    "    patience = 5,\n",
    "    verbose = 1,\n",
    "    mode = 'min',\n",
    "    restore_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:04.931173Z",
     "iopub.status.busy": "2025-04-04T03:59:04.930890Z",
     "iopub.status.idle": "2025-04-04T03:59:04.935069Z",
     "shell.execute_reply": "2025-04-04T03:59:04.934222Z",
     "shell.execute_reply.started": "2025-04-04T03:59:04.931153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the learning rate schedule\n",
    "initial_learning_rate = 0.0002\n",
    "decay_steps = 50 * 300   # decay over 100 epochs (from 100 to 200)\n",
    "end_learning_rate = 0.0\n",
    "\n",
    "# Create the linear decay schedule\n",
    "lr_schedule = PolynomialDecay(\n",
    "    initial_learning_rate = initial_learning_rate,\n",
    "    decay_steps = decay_steps,\n",
    "    end_learning_rate = end_learning_rate,\n",
    "    power = 1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:05.248483Z",
     "iopub.status.busy": "2025-04-04T03:59:05.248144Z",
     "iopub.status.idle": "2025-04-04T03:59:08.229292Z",
     "shell.execute_reply": "2025-04-04T03:59:08.228652Z",
     "shell.execute_reply.started": "2025-04-04T03:59:05.248456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "CycleGan2 = CycleGAN(generator_monet = generator(name = 'gen_monet'), \n",
    "                    generator_photo = generator(name = 'gen_photo'), \n",
    "                    discriminator_monet = discriminator(name = 'dis_monet'), \n",
    "                    discriminator_photo = discriminator(name = 'dis_photo'))\n",
    "    \n",
    "CycleGan2.compile(genM_optimizer = Adam(learning_rate=lr_schedule), \n",
    "                 genP_optimizer = Adam(learning_rate=lr_schedule), \n",
    "                 disM_optimizer = Adam(learning_rate=lr_schedule), \n",
    "                 disP_optimizer = Adam(learning_rate=lr_schedule),\n",
    "                 gen_loss = generator_loss, \n",
    "                 dis_loss = discriminator_loss, \n",
    "                 cycle_loss = cycle_loss, \n",
    "                 identity_loss = identity_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T03:59:08.230771Z",
     "iopub.status.busy": "2025-04-04T03:59:08.230478Z",
     "iopub.status.idle": "2025-04-04T07:26:28.056911Z",
     "shell.execute_reply": "2025-04-04T07:26:28.056093Z",
     "shell.execute_reply.started": "2025-04-04T03:59:08.230744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the weights from the first CycleGan model\n",
    "CycleGan2.load_weights('/kaggle/input/Monet-cyclegan-1/cyclegan1.weights.h5')\n",
    "\n",
    "history = CycleGan2.fit(CycleGan_ds, \n",
    "                       steps_per_epoch = 300,\n",
    "                       epochs = 150,\n",
    "                       callbacks = [check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:28:17.533751Z",
     "iopub.status.busy": "2025-04-04T07:28:17.533427Z",
     "iopub.status.idle": "2025-04-04T07:28:19.902507Z",
     "shell.execute_reply": "2025-04-04T07:28:19.901613Z",
     "shell.execute_reply.started": "2025-04-04T07:28:17.533727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract generator_monet\n",
    "generator_monet =  CycleGan2.GenM\n",
    "\n",
    "# Visualize the Photos vs. Monet-style photos \n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 3, figsize = (15, 10))\n",
    "fig.suptitle('Photo vs. Monet-style', fontsize = 16)\n",
    "for i, img in enumerate(photo_ds.take(3)):\n",
    "    generated_img = generator_monet(img, training = False)[0].numpy()\n",
    "    generated_img = (generated_img * 127.5 + 127.5).astype(np.uint8)\n",
    "    img = tf.cast(img * 127.5 + 127.5, tf.uint8).numpy()\n",
    "    img = np.squeeze(img, axis=0)\n",
    "    ax[0, i].imshow(img)\n",
    "    ax[0, i].set_title('Photo')\n",
    "    ax[0, i].axis('off')\n",
    "    ax[1, i].imshow(generated_img)\n",
    "    ax[1, i].set_title('Monet-style')\n",
    "    ax[1, i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the Generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T07:29:21.555226Z",
     "iopub.status.busy": "2025-04-04T07:29:21.554928Z",
     "iopub.status.idle": "2025-04-04T07:43:22.217243Z",
     "shell.execute_reply": "2025-04-04T07:43:22.216480Z",
     "shell.execute_reply.started": "2025-04-04T07:29:21.555204Z"
    }
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import shutil\n",
    "\n",
    "# Create the folder to save generated images\n",
    "os.makedirs('../images/')\n",
    "\n",
    "# Generate monet-style images\n",
    "i = 1\n",
    "for img in photo_ds:\n",
    "    generated_img = generator_monet(img, training = False)[0].numpy()\n",
    "    generated_img = (generated_img * 127.5 + 127.5).astype(np.uint8)\n",
    "    im = PIL.Image.fromarray(generated_img)\n",
    "    im.save(\"../images/\" + str(i) + '.jpg')\n",
    "    i += 1\n",
    "\n",
    "print(f\"Generated images: {len([name for name in os.listdir('../images') if os.path.isfile(os.path.join('../images', name))])}\")\n",
    "\n",
    "# archive the image folder\n",
    "shutil.make_archive('/kaggle/working/images', 'zip', '../images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project, I built a CycleGAN model to translate regular photos into Monet-style paintings. By carefully following the details in the original paper, learning from the CycleGAN example in Keras, and studying notebooks from other Kagglers, I successfully built and trained the model on the Kaggle dataset.\n",
    "\n",
    "The Monet-style images generated by the second model (trained for 200 epochs in total) appear more realistic than those from the first one (trained for 50 epochs, see **Monet-cyclegan-1**.), indicating that additional training epochs led to better results in this case.\n",
    "\n",
    "As I am completely new to GANs, this project was more of a learning process for me on how to build a CycleGAN model using the Keras API. In future projects, I may explore further improvements by fine-tuning more parameters or adopting other deep neural network techniques.  \n",
    "\n",
    "### Reference¶\n",
    "Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. 2020. arXiv:1703.10593 [cs.CV]\n",
    "\n",
    "Amy Jang. Monet CycleGAN Tutorial. Kaggle Notebook. https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Github Link :** https://github.com/kirkboyd1/GANs/blob/main/Monet-cyclegan-2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1475600,
     "sourceId": 21755,
     "sourceType": "competition"
    },
    {
     "sourceId": 226855511,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
